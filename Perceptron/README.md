# Implementa√ß√£o de Perceptron para Classifica√ß√£o Bin√°ria - Dataset Iris

## üìã Descri√ß√£o do Projeto

Este projeto implementa um **perceptron simples** do zero para realizar classifica√ß√£o bin√°ria no dataset Iris. O objetivo √© distinguir entre duas esp√©cies espec√≠ficas de flores Iris (Setosa e Virginica) utilizando caracter√≠sticas morfol√≥gicas como comprimento e largura das s√©palas e p√©talas.

O projeto demonstra os conceitos fundamentais de **redes neurais artificiais**, incluindo:
- Implementa√ß√£o manual de um perceptron
- Fun√ß√£o de ativa√ß√£o sigm√≥ide
- Algoritmo de treinamento iterativo
- Visualiza√ß√£o de fronteiras de decis√£o
- Pr√©-processamento e normaliza√ß√£o de dados

## üß† Explica√ß√£o T√©cnica

### Aprendizado de M√°quina

**Aprendizado de M√°quina** √© uma sub√°rea da intelig√™ncia artificial que permite aos computadores aprenderem padr√µes nos dados sem serem explicitamente programados. Neste projeto, utilizamos:

- **Aprendizado Supervisionado**: O algoritmo aprende a partir de exemplos rotulados
- **Classifica√ß√£o Bin√°ria**: Problema onde o objetivo √© classificar dados em uma de duas categorias
- **Perceptron**: O modelo mais simples de rede neural, capaz de aprender fun√ß√µes linearmente separ√°veis

### Funcionamento do Perceptron

1. **Entrada e Pesos**:
   - Dados de entrada: caracter√≠sticas das flores (4 dimens√µes)
   - Pesos: valores aleat√≥rios inicializados para cada caracter√≠stica
   - Bias: termo constante para ajustar a fun√ß√£o de decis√£o

2. **C√°lculo da Sa√≠da**:
   - Combina√ß√£o linear: `Z = X¬∑W + b`
   - Fun√ß√£o de ativa√ß√£o sigm√≥ide: `œÉ(Z) = 1/(1 + e^(-Z))`
   - Sa√≠da: valor entre 0 e 1 representando a probabilidade da classe

3. **Treinamento**:
   - Compara√ß√£o entre sa√≠da prevista e valor real
   - Ajuste iterativo dos pesos e bias
   - Converg√™ncia para uma solu√ß√£o √≥tima

### Dataset Iris

O dataset cont√©m 150 amostras com 4 caracter√≠sticas:
- **Sepal Length** (comprimento da s√©pala)
- **Sepal Width** (largura da s√©pala)
- **Petal Length** (comprimento da p√©tala)
- **Petal Width** (largura da p√©tala)

Para classifica√ß√£o bin√°ria, filtramos apenas **Iris Setosa** (classe 0) e **Iris Virginica** (classe 2, recodificada como 1).

## ‚öôÔ∏è Pr√©-requisitos

Para executar este projeto, voc√™ precisar√° de:

- **Python 3.7+**
- **Jupyter Notebook** ou **Google Colab**
- **Conhecimentos b√°sicos** de:
  - Python e NumPy
  - √Ålgebra linear (produto escalar, matrizes)
  - Conceitos fundamentais de machine learning
  - Bibliotecas Matplotlib e Scikit-learn

## üöÄ Instala√ß√£o

### Op√ß√£o 1: Google Colab (Recomendado)
1. Acesse o link do notebook no Google Colab
2. Fa√ßa uma c√≥pia para sua conta Google
3. Execute as c√©lulas sequencialmente

### Op√ß√£o 2: Ambiente Local
1. **Clone o reposit√≥rio**:
   ```bash
   git clone https://github.com/nicolasquant/inteligencia_artificial.git
   cd inteligencia_artificial/Perceptron
   ```

2. **Instale as depend√™ncias**:
   ```bash
   pip install numpy matplotlib scikit-learn jupyter
   ```

3. **Execute o Jupyter Notebook**:
   ```bash
   jupyter notebook Perceptron.ipynb
   ```

## üìñ Instru√ß√µes de Uso

### 1. **Estrutura do Notebook**
O notebook est√° organizado em se√ß√µes l√≥gicas:

- **PARTE 0**: Tratamento e prepara√ß√£o dos dados
- **PARTE 1**: Implementa√ß√£o das fun√ß√µes do perceptron
- **PARTE 2**: Treinamento e visualiza√ß√£o do modelo

### 2. **Execu√ß√£o Sequencial**
Execute as c√©lulas na ordem apresentada:

1. **Importa√ß√£o e carregamento de dados**
2. **Pr√©-processamento** (filtragem, divis√£o treino/teste, normaliza√ß√£o)
3. **Inicializa√ß√£o dos par√¢metros** (pesos e bias)
4. **Implementa√ß√£o das fun√ß√µes** (c√°lculo de Z, fun√ß√£o sigm√≥ide)
5. **Fun√ß√£o de visualiza√ß√£o** (fronteiras de decis√£o)
6. **Treinamento do modelo**
7. **Avalia√ß√£o e visualiza√ß√£o dos resultados**

### 3. **Par√¢metros Configur√°veis**
- **`eta = 0.01`**: Coeficiente de aprendizado
- **`test_size = 0.2`**: Propor√ß√£o de dados para teste
- **`random_state = 42`**: Semente para reprodutibilidade
- **N√∫mero de itera√ß√µes**: Ajust√°vel conforme necess√°rio

### 4. **Interpreta√ß√£o dos Resultados**
- **Gr√°ficos de dispers√£o**: Mostram a distribui√ß√£o das classes
- **Fronteiras de decis√£o**: Visualizam como o perceptron classifica os dados
- **Pesos finais**: Representam o modelo treinado
- **Acur√°cia**: Medida de performance do classificador

### 5. **Personaliza√ß√µes Poss√≠veis**
- Alterar a taxa de aprendizado
- Modificar o n√∫mero de itera√ß√µes
- Testar diferentes fun√ß√µes de ativa√ß√£o
- Implementar valida√ß√£o cruzada
- Adicionar regulariza√ß√£o

## üîç Estrutura do C√≥digo

```
‚îú‚îÄ‚îÄ Importa√ß√£o de bibliotecas
‚îú‚îÄ‚îÄ Carregamento do dataset Iris
‚îú‚îÄ‚îÄ Filtragem para classifica√ß√£o bin√°ria
‚îú‚îÄ‚îÄ Divis√£o treino/teste
‚îú‚îÄ‚îÄ Normaliza√ß√£o dos dados
‚îú‚îÄ‚îÄ Inicializa√ß√£o dos par√¢metros
‚îú‚îÄ‚îÄ Implementa√ß√£o das fun√ß√µes
‚îú‚îÄ‚îÄ Treinamento do modelo
‚îî‚îÄ‚îÄ Visualiza√ß√£o dos resultados
```

## üìä Resultados Esperados

Ap√≥s a execu√ß√£o, voc√™ ver√°:
- Gr√°ficos mostrando a separa√ß√£o das duas classes
- Fronteiras de decis√£o que se ajustam durante o treinamento
- Modelo treinado capaz de classificar novas amostras
- Pesos finais que representam a solu√ß√£o aprendida

## üéØ Aplica√ß√µes Pr√°ticas

Este projeto serve como base para:
- Entender o funcionamento b√°sico de redes neurais
- Implementar algoritmos de classifica√ß√£o simples
- Aprender sobre pr√©-processamento de dados
- Visualizar o processo de aprendizado
- Aplicar conceitos em problemas reais de classifica√ß√£o

## ü§ù Contribui√ß√µes

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para:
- Reportar bugs
- Sugerir melhorias
- Adicionar novas funcionalidades
- Melhorar a documenta√ß√£o

## üìö Refer√™ncias

- [Scikit-learn Documentation](https://scikit-learn.org/)
- [NumPy Documentation](https://numpy.org/)
- [Matplotlib Documentation](https://matplotlib.org/)
- [Dataset Iris - UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/iris)

---

**Desenvolvido por**: Nicolas Quant  
**Data**: 2024  
**Licen√ßa**: MIT
