# Implementa√ß√£o do Algoritmo K-Means para Clustering - Dataset Iris

## üìã Descri√ß√£o do Projeto

Este projeto implementa o algoritmo **K-Means** do zero para realizar **clustering** (agrupamento) de dados no dataset Iris. O objetivo √© agrupar as flores em 3 clusters baseando-se nas caracter√≠sticas das p√©talas (comprimento e largura), demonstrando como o algoritmo identifica padr√µes e organiza dados similares em grupos.

O projeto demonstra os conceitos fundamentais de **aprendizado de m√°quina n√£o supervisionado**, incluindo:
- Implementa√ß√£o manual do algoritmo K-Means
- Sele√ß√£o de centroides iniciais aleat√≥rios
- C√°lculo de dist√¢ncias euclidianas
- Atualiza√ß√£o iterativa de centroides
- Avalia√ß√£o de qualidade atrav√©s de vari√¢ncia
- Visualiza√ß√£o de clusters e centroides

## üß† Explica√ß√£o T√©cnica

### Aprendizado de M√°quina

**Aprendizado de M√°quina** √© uma sub√°rea da intelig√™ncia artificial que permite aos computadores aprenderem padr√µes nos dados sem serem explicitamente programados. Neste projeto, utilizamos:

- **Aprendizado N√£o Supervisionado**: O algoritmo descobre padr√µes nos dados sem r√≥tulos pr√©-definidos
- **Clustering**: T√©cnica para agrupar dados similares em clusters
- **K-Means**: Algoritmo iterativo que minimiza a vari√¢ncia dentro de cada cluster
- **An√°lise Explorat√≥ria**: Descoberta de estrutura nos dados

### Funcionamento do Algoritmo K-Means

1. **Inicializa√ß√£o**:
   - Define o n√∫mero de clusters (K = 3)
   - Seleciona K pontos aleat√≥rios como centroides iniciais
   - Utiliza caracter√≠sticas das p√©talas (comprimento e largura)

2. **Atribui√ß√£o de Pontos**:
   - Calcula dist√¢ncia euclidiana de cada ponto aos centroides
   - Atribui cada ponto ao cluster do centroide mais pr√≥ximo
   - Formula: `d = ‚àö((x‚ÇÅ-y‚ÇÅ)¬≤ + (x‚ÇÇ-y‚ÇÇ)¬≤)`

3. **Atualiza√ß√£o de Centroides**:
   - Calcula a m√©dia de todos os pontos em cada cluster
   - Move o centroide para a nova posi√ß√£o m√©dia
   - Repete at√© converg√™ncia ou n√∫mero m√°ximo de itera√ß√µes

4. **Avalia√ß√£o de Qualidade**:
   - Calcula vari√¢ncia dentro de cada cluster
   - Soma total das vari√¢ncias como m√©trica de qualidade
   - Menor vari√¢ncia total indica melhor agrupamento

### Dataset Iris

O dataset cont√©m 150 amostras de flores com 4 caracter√≠sticas:
- **Sepal Length** (comprimento da s√©pala)
- **Sepal Width** (largura da s√©pala)
- **Petal Length** (comprimento da p√©tala) ‚Üê **Utilizado**
- **Petal Width** (largura da p√©tala) ‚Üê **Utilizado**

Para este projeto, utilizamos apenas as caracter√≠sticas das **p√©talas** (colunas 2 e 3) para facilitar a visualiza√ß√£o 2D.

## ‚öôÔ∏è Pr√©-requisitos

Para executar este projeto, voc√™ precisar√° de:

- **Python 3.7+**
- **Jupyter Notebook** ou **Google Colab**
- **Conhecimentos b√°sicos** de:
  - Python e NumPy
  - √Ålgebra linear (c√°lculo de dist√¢ncias, m√©dias)
  - Conceitos fundamentais de machine learning
  - Bibliotecas Matplotlib e Scikit-learn
  - Algoritmos de ordena√ß√£o (QuickSort)

## üöÄ Instala√ß√£o

### Op√ß√£o 1: Google Colab (Recomendado)
1. Acesse o link do notebook no Google Colab
2. Fa√ßa uma c√≥pia para sua conta Google
3. Execute as c√©lulas sequencialmente

### Op√ß√£o 2: Ambiente Local
1. **Clone o reposit√≥rio**:
   ```bash
   git clone https://github.com/nicolasquant/inteligencia_artificial.git
   cd inteligencia_artificial
   ```

2. **Instale as depend√™ncias**:
   ```bash
   pip install numpy matplotlib scikit-learn jupyter
   ```

3. **Execute o Jupyter Notebook**:
   ```bash
   jupyter notebook kmeans.ipynb
   ```

## üìñ Instru√ß√µes de Uso

### 1. **Estrutura do Notebook**
O notebook est√° organizado em se√ß√µes l√≥gicas:

- **Importa√ß√£o e prepara√ß√£o de dados**
- **Implementa√ß√£o do QuickSort**
- **Implementa√ß√£o do K-Means**
- **Execu√ß√£o e avalia√ß√£o**
- **Visualiza√ß√£o dos resultados**

### 2. **Execu√ß√£o Sequencial**
Execute as c√©lulas na ordem apresentada:

1. **Importa√ß√£o de bibliotecas e carregamento de dados**
2. **Implementa√ß√£o do algoritmo QuickSort** (para ordena√ß√£o)
3. **Pr√©-processamento dos dados** (divis√£o treino/teste, normaliza√ß√£o)
4. **Visualiza√ß√£o inicial** dos dados com classes reais
5. **Implementa√ß√£o das fun√ß√µes auxiliares**:
   - `random_points()`: Sele√ß√£o aleat√≥ria de centroides
   - `distancia_euclidiana()`: C√°lculo de dist√¢ncia entre pontos
6. **Execu√ß√£o do K-Means** com m√∫ltiplas inicializa√ß√µes
7. **Sele√ß√£o da melhor solu√ß√£o** baseada na vari√¢ncia
8. **Visualiza√ß√£o final** dos clusters e centroides

### 3. **Par√¢metros Configur√°veis**
- **`k = 3`**: N√∫mero de clusters (classes de flores)
- **`test_size = 0.2`**: Propor√ß√£o de dados para teste (20%)
- **`random_state = 42`**: Semente para reprodutibilidade
- **`10`**: N√∫mero de inicializa√ß√µes aleat√≥rias para encontrar melhor solu√ß√£o

### 4. **Implementa√ß√£o das Fun√ß√µes**

#### QuickSort para Ordena√ß√£o
```python
def quicksort(arr, left, right):
    if left < right:
        partition_pos = partition(arr, left, right)
        quicksort(arr, left, partition_pos - 1)
        quicksort(arr, partition_pos + 1, right)
```

#### Sele√ß√£o de Centroides Aleat√≥rios
```python
def random_points(matriz_de_dados, k):
    pontos = []
    for i in range(k):
        coordenada = int(len(matriz_de_dados)*np.random.random())
        pontos.append(matriz_de_dados[coordenada])
    return pontos
```

#### C√°lculo de Dist√¢ncia Euclidiana
```python
def distancia_euclidiana(ponto1, ponto2):
    x = ponto1[0] - ponto2[0]
    y = ponto1[1] - ponto2[1]
    return np.sqrt(x**2 + y**2)
```

### 5. **Processo de Otimiza√ß√£o**
1. **M√∫ltiplas Inicializa√ß√µes**: Gera 10 conjuntos diferentes de centroides aleat√≥rios
2. **Execu√ß√£o do K-Means**: Para cada inicializa√ß√£o, executa o algoritmo completo
3. **C√°lculo de Vari√¢ncia**: Mede a qualidade de cada agrupamento
4. **Sele√ß√£o da Melhor Solu√ß√£o**: Escolhe a inicializa√ß√£o com menor vari√¢ncia total
5. **Re-execu√ß√£o**: Aplica o K-Means com os melhores centroides iniciais

### 6. **Interpreta√ß√£o dos Resultados**
- **Gr√°ficos de dispers√£o**: Mostram a distribui√ß√£o dos pontos por cluster
- **Centroides**: Pontos m√©dios de cada cluster (marcados com cores diferentes)
- **Qualidade do agrupamento**: Avaliada pela vari√¢ncia total dos clusters
- **Compara√ß√£o**: Visualiza√ß√£o dos dados reais vs. clusters encontrados

### 7. **Personaliza√ß√µes Poss√≠veis**
- Alterar o n√∫mero de clusters (K)
- Modificar o n√∫mero de inicializa√ß√µes aleat√≥rias
- Implementar crit√©rios de parada mais sofisticados
- Adicionar outras m√©tricas de qualidade (Silhouette Score, Inertia)
- Testar diferentes m√©todos de inicializa√ß√£o (K-Means++)

## üîç Estrutura do C√≥digo

```
‚îú‚îÄ‚îÄ Importa√ß√£o de bibliotecas
‚îú‚îÄ‚îÄ Implementa√ß√£o do QuickSort
‚îú‚îÄ‚îÄ Carregamento do dataset Iris
‚îú‚îÄ‚îÄ Sele√ß√£o de caracter√≠sticas (p√©talas)
‚îú‚îÄ‚îÄ Divis√£o treino/teste
‚îú‚îÄ‚îÄ Normaliza√ß√£o dos dados
‚îú‚îÄ‚îÄ Visualiza√ß√£o inicial
‚îú‚îÄ‚îÄ Implementa√ß√£o das fun√ß√µes auxiliares
‚îú‚îÄ‚îÄ Execu√ß√£o do K-Means (m√∫ltiplas inicializa√ß√µes)
‚îú‚îÄ‚îÄ Sele√ß√£o da melhor solu√ß√£o
‚îú‚îÄ‚îÄ Re-execu√ß√£o com melhores centroides
‚îî‚îÄ‚îÄ Visualiza√ß√£o final dos resultados
```

## üìä Resultados Esperados

Ap√≥s a execu√ß√£o, voc√™ ver√°:
- Gr√°fico inicial mostrando as 3 classes reais do dataset
- Gr√°fico final mostrando os 3 clusters encontrados pelo K-Means
- Centroides de cada cluster marcados com cores diferentes
- Compara√ß√£o entre agrupamento real e agrupamento aprendido
- M√©tricas de qualidade baseadas na vari√¢ncia dos clusters

## üéØ Aplica√ß√µes Pr√°ticas

Este projeto serve como base para:
- Entender algoritmos de clustering n√£o supervisionado
- Implementar K-Means do zero
- Aprender sobre inicializa√ß√£o de centroides
- Visualizar o processo de agrupamento
- Aplicar conceitos em problemas reais de segmenta√ß√£o
- Entender a import√¢ncia da inicializa√ß√£o em algoritmos iterativos

## ‚ö†Ô∏è Considera√ß√µes Importantes

- **Inicializa√ß√£o Aleat√≥ria**: Diferentes execu√ß√µes podem produzir resultados diferentes
- **Converg√™ncia Local**: O algoritmo pode convergir para m√≠nimos locais
- **Escolha de K**: O n√∫mero de clusters deve ser conhecido a priori
- **Escala dos Dados**: Normaliza√ß√£o √© importante para dist√¢ncias euclidianas
- **M√∫ltiplas Execu√ß√µes**: Executar v√°rias vezes ajuda a encontrar melhor solu√ß√£o

## üîç Caracter√≠sticas Especiais

Esta implementa√ß√£o se destaca por:
- **Implementa√ß√£o Manual**: Algoritmo desenvolvido do zero para aprendizado
- **M√∫ltiplas Inicializa√ß√µes**: 10 execu√ß√µes para encontrar melhor solu√ß√£o
- **Avalia√ß√£o por Vari√¢ncia**: M√©trica simples mas efetiva para qualidade
- **QuickSort Personalizado**: Algoritmo de ordena√ß√£o implementado manualmente
- **Visualiza√ß√£o Completa**: Gr√°ficos antes e depois do clustering

## ü§ù Contribui√ß√µes

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para:
- Reportar bugs
- Sugerir melhorias
- Adicionar novas funcionalidades
- Melhorar a documenta√ß√£o
- Implementar outras m√©tricas de avalia√ß√£o

## üìö Refer√™ncias

- [Scikit-learn Documentation](https://scikit-learn.org/)
- [NumPy Documentation](https://numpy.org/)
- [Matplotlib Documentation](https://matplotlib.org/)
- [Dataset Iris - UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/iris)
- [K-Means Clustering Algorithm](https://en.wikipedia.org/wiki/K-means_clustering)

---

**Desenvolvido por**: Nicolas Quant  
**Data**: 2024  
**Licen√ßa**: MIT
