# Classifica√ß√£o de Vinhos com K-Nearest Neighbors (KNN) - Implementa√ß√£o Gubio

## üìã Descri√ß√£o do Projeto

Este projeto implementa o algoritmo **K-Nearest Neighbors (KNN)** de forma simplificada para classificar vinhos como "bons" ou "ruins" baseando-se em caracter√≠sticas qu√≠micas e f√≠sicas. O objetivo √© determinar a qualidade do vinho atrav√©s de an√°lise de 11 atributos diferentes, utilizando uma implementa√ß√£o direta e eficiente do algoritmo KNN.

O projeto demonstra os conceitos fundamentais de **aprendizado de m√°quina supervisionado**, incluindo:
- Implementa√ß√£o direta do algoritmo KNN
- Pr√©-processamento e normaliza√ß√£o Min-Max
- Divis√£o em conjuntos de treino e valida√ß√£o
- C√°lculo de dist√¢ncias euclidianas
- Avalia√ß√£o de performance com m√©tricas de acur√°cia

## üß† Explica√ß√£o T√©cnica

### Aprendizado de M√°quina

**Aprendizado de M√°quina** √© uma sub√°rea da intelig√™ncia artificial que permite aos computadores aprenderem padr√µes nos dados sem serem explicitamente programados. Neste projeto, utilizamos:

- **Aprendizado Supervisionado**: O algoritmo aprende a partir de exemplos rotulados (vinhos classificados como bons ou ruins)
- **Classifica√ß√£o Bin√°ria**: Problema onde o objetivo √© classificar vinhos em uma de duas categorias
- **K-Nearest Neighbors (KNN)**: Algoritmo baseado em similaridade que classifica novos dados baseando-se nos K vizinhos mais pr√≥ximos

### Funcionamento do Algoritmo KNN

1. **C√°lculo de Dist√¢ncias**:
   - Para cada amostra de valida√ß√£o, calcula a dist√¢ncia euclidiana at√© todas as amostras de treino
   - Dist√¢ncia euclidiana: `d = ‚àö(Œ£(x‚ÇÅ-y‚ÇÅ)¬≤ + (x‚ÇÇ-y‚ÇÇ)¬≤ + ... + (x‚Çô-y‚Çô)¬≤)`

2. **Sele√ß√£o dos K Vizinhos**:
   - Identifica os K pontos de treino mais pr√≥ximos da amostra de valida√ß√£o
   - K √© um hiperpar√¢metro que determina quantos vizinhos considerar

3. **Classifica√ß√£o por Vota√ß√£o**:
   - Conta a frequ√™ncia de cada classe entre os K vizinhos mais pr√≥ximos
   - A classe mais frequente se torna a predi√ß√£o para a nova amostra

### Dataset de Vinhos

O dataset cont√©m informa√ß√µes sobre vinhos com 11 caracter√≠sticas:
- **Fixed Acidity** (acidez fixa)
- **Volatile Acidity** (acidez vol√°til)
- **Citric Acid** (√°cido c√≠trico)
- **Residual Sugar** (a√ß√∫car residual)
- **Chlorides** (cloretos)
- **Free Sulfur Dioxide** (di√≥xido de enxofre livre)
- **Total Sulfur Dioxide** (di√≥xido de enxofre total)
- **Density** (densidade)
- **pH** (potencial hidrogeni√¥nico)
- **Sulphates** (sulfatos)
- **Alcohol** (teor alco√≥lico)

**Target**: Coluna "is good" indicando se o vinho √© bom (1) ou ruim (0).

## ‚öôÔ∏è Pr√©-requisitos

Para executar este projeto, voc√™ precisar√° de:

- **Python 3.7+**
- **Jupyter Notebook** ou **Google Colab**
- **Arquivo de dados**: `vinho.csv` (deve estar no mesmo diret√≥rio)
- **Conhecimentos b√°sicos** de:
  - Python e Pandas
  - √Ålgebra linear (c√°lculo de dist√¢ncias)
  - Conceitos fundamentais de machine learning
  - Bibliotecas NumPy e Scikit-learn

## üöÄ Instala√ß√£o

### Op√ß√£o 1: Google Colab (Recomendado)
1. Acesse o link do notebook no Google Colab
2. Fa√ßa uma c√≥pia para sua conta Google
3. **Importante**: Fa√ßa upload do arquivo `vinho.csv` para o Colab
4. Execute as c√©lulas sequencialmente

### Op√ß√£o 2: Ambiente Local
1. **Clone o reposit√≥rio**:
   ```bash
   git clone https://github.com/nicolasquant/inteligencia_artificial.git
   cd inteligencia_artificial
   ```

2. **Instale as depend√™ncias**:
   ```bash
   pip install numpy pandas scikit-learn jupyter
   ```

3. **Certifique-se de ter o arquivo `vinho.csv`** no diret√≥rio
4. **Execute o Jupyter Notebook**:
   ```bash
   jupyter notebook ML_knn_G.ipynb
   ```

## üìñ Instru√ß√µes de Uso

### 1. **Prepara√ß√£o dos Dados**
- **Carregamento**: O notebook carrega automaticamente o arquivo `vinho.csv`
- **Explora√ß√£o**: Use `df.head()` para visualizar as primeiras linhas
- **Verifica√ß√£o**: Confirme que todas as 11 caracter√≠sticas est√£o presentes

### 2. **Execu√ß√£o Sequencial**
Execute as c√©lulas na ordem apresentada:

1. **Importa√ß√£o e carregamento de dados**
2. **Normaliza√ß√£o Min-Max** (escala [0,1])
3. **Separa√ß√£o de features e target**
4. **Divis√£o treino/valida√ß√£o**
5. **Implementa√ß√£o direta do KNN**
6. **C√°lculo de dist√¢ncias e classifica√ß√£o**
7. **Avalia√ß√£o da performance**

### 3. **Par√¢metros Configur√°veis**
- **`test_size = 0.2`**: Propor√ß√£o de dados para valida√ß√£o (20%)
- **`random_state = 42`**: Semente para reprodutibilidade
- **`k = 3`**: N√∫mero de vizinhos (hiperpar√¢metro principal)
- **Fun√ß√£o de normaliza√ß√£o**: Min-Max Scaling para escala [0,1]

### 4. **Implementa√ß√£o Direta do KNN**
O algoritmo √© implementado de forma direta:
- **Loop externo**: Para cada amostra de valida√ß√£o
- **Loop interno**: Calcula dist√¢ncia at√© todas as amostras de treino
- **Sele√ß√£o**: Identifica os K vizinhos mais pr√≥ximos
- **Vota√ß√£o**: Classifica baseado na classe mais frequente

### 5. **Fun√ß√£o de Acur√°cia Personalizada**
```python
def accuracy(y_val, y_pred):
    total = 0
    for val, pred in zip(y_val, y_pred):
        if (val == pred):
            total += 1
    return total / len(y_val)
```

### 6. **Interpreta√ß√£o dos Resultados**
- **Taxa de acerto**: Porcentagem de predi√ß√µes corretas
- **Compara√ß√£o direta**: Valores reais vs. preditos
- **Influ√™ncia do par√¢metro K**: Como o n√∫mero de vizinhos afeta a performance

### 7. **Personaliza√ß√µes Poss√≠veis**
- Testar diferentes valores de K (1, 3, 5, 7, etc.)
- Implementar outras m√©tricas de dist√¢ncia
- Adicionar valida√ß√£o cruzada
- Implementar pondera√ß√£o por dist√¢ncia
- Testar diferentes t√©cnicas de normaliza√ß√£o

## üîç Estrutura do C√≥digo

```
‚îú‚îÄ‚îÄ Importa√ß√£o de bibliotecas
‚îú‚îÄ‚îÄ Carregamento do dataset de vinhos
‚îú‚îÄ‚îÄ Normaliza√ß√£o Min-Max dos dados
‚îú‚îÄ‚îÄ Separa√ß√£o de features e target
‚îú‚îÄ‚îÄ Divis√£o treino/valida√ß√£o
‚îú‚îÄ‚îÄ Implementa√ß√£o direta do KNN
‚îú‚îÄ‚îÄ C√°lculo de dist√¢ncias
‚îú‚îÄ‚îÄ Classifica√ß√£o por vota√ß√£o
‚îî‚îÄ‚îÄ Avalia√ß√£o da performance
```

## üìä Resultados Esperados

Ap√≥s a execu√ß√£o, voc√™ ver√°:
- Dataset normalizado com valores entre 0 e 1
- Taxa de acerto do classificador KNN
- Compara√ß√£o entre valores reais e preditos
- An√°lise da influ√™ncia do par√¢metro K na performance

## üéØ Aplica√ß√µes Pr√°ticas

Este projeto serve como base para:
- Entender algoritmos baseados em similaridade
- Implementar classificadores KNN de forma direta
- Aprender sobre pr√©-processamento de dados
- Aplicar machine learning em problemas de classifica√ß√£o
- Entender a import√¢ncia da normaliza√ß√£o de dados

## ‚ö†Ô∏è Considera√ß√µes Importantes

- **Arquivo de dados**: Certifique-se de ter o arquivo `vinho.csv`
- **Normaliza√ß√£o**: Dados devem estar na mesma escala para c√°lculo correto de dist√¢ncias
- **Par√¢metro K**: Valores muito baixos podem causar overfitting, valores muito altos podem causar underfitting
- **Performance**: KNN pode ser computacionalmente custoso para datasets grandes
- **Implementa√ß√£o**: Esta vers√£o √© mais direta e did√°tica, ideal para aprendizado

## üîç Diferen√ßas da Vers√£o Gubio

Esta implementa√ß√£o se destaca por:
- **Simplicidade**: C√≥digo mais direto e f√°cil de entender
- **Efici√™ncia**: Implementa√ß√£o otimizada para datasets menores
- **Did√°tica**: Ideal para aprendizado dos conceitos fundamentais
- **Flexibilidade**: F√°cil de modificar e experimentar

## ü§ù Contribui√ß√µes

Contribui√ß√µes s√£o bem-vindas! Sinta-se √† vontade para:
- Reportar bugs
- Sugerir melhorias
- Adicionar novas funcionalidades
- Melhorar a documenta√ß√£o

## üìö Refer√™ncias

- [Scikit-learn Documentation](https://scikit-learn.org/)
- [Pandas Documentation](https://pandas.pydata.org/)
- [NumPy Documentation](https://numpy.org/)
- [K-Nearest Neighbors Algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)

---

**Desenvolvido por**: Gubio (Colabora√ß√£o com Nicolas Quant)  
**Data**: 2024  
**Licen√ßa**: MIT
